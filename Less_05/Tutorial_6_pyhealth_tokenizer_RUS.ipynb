{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximTislenko/DS_in_MED/blob/main/Less_05/Tutorial_6_pyhealth_tokenizer_RUS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Подготовка**\n",
        "- установить альфа-версию pyhealth"
      ],
      "metadata": {
        "id": "PHPVEkPFKYkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbz1YJ2nqOSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143bda64-b39a-42a7-ec0a-01ebabfe9f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhealth\n",
            "  Downloading pyhealth-1.1.3-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pyhealth) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.13.1+cu116)\n",
            "Collecting rdkit>=2022.03.4\n",
            "  Downloading rdkit-2022.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.3/29.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit>=2022.03.4->pyhealth) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->pyhealth) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.2->pyhealth) (1.15.0)\n",
            "Installing collected packages: rdkit, pyhealth\n",
            "Successfully installed pyhealth-1.1.3 rdkit-2022.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pyhealth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Инструкция по [pyhealth.tokenizer](https://pyhealth.readthedocs.io/en/latest/api/tokenizer.html)**\n",
        "- **[README]**: токинайзер используется для преобразований между строковыми токенами и целочисленными индексами на основе общего пространства токенов. Мы предоставляем гибкие функции для токенизации списков 1D, 2D и 3D. **Этот модуль можно использовать во многих других сценариях.**\n",
        "\n",
        "- **[Arguments]**:\n",
        "  - `tokens`: Список токенов в словаре.\n",
        "  - `special_tokens`: Список специальных токенов, которые можно добавить в словарь (такие как, <pad>, <unk>). Если он не указан, специальные токены не добавляются\n",
        "\n",
        "- **[Functionality]**:\n",
        "  - `get_vocabulary_size`: Вернуть размер словаря\n",
        "  - `convert_tokens_to_indices`: 1d конвертация токенов в индексы\n",
        "  - `convert_indices_to_tokens`: 1d конвертация индексов в токены\n",
        "  - `batch_encode_2d`: 2d конвертация токенов в индексы\n",
        "  - `batch_decode_2d`: 2d конвертация индексов в токены\n",
        "  - `batch_encode_3d`: 3d конвертация токенов в индексы\n",
        "  - `batch_decode_3d`: 3d конвертация токенов в индексы"
      ],
      "metadata": {
        "id": "_1S5rqae7FhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Пример 1: 1D токенизация**\n",
        "- Мы предоставляем примеры одномерного преобразования между токенами и индексами.\n",
        "- Мы используем `[\"<pad>\", \"<unk>\"]` два специальных токена, `<pad>`используется для заполнения при многомерном кодировании и декодировании, и `<unk>` используется для неизвестных токенов. При токенизации 1d токен <pad> бесполезен.."
      ],
      "metadata": {
        "id": "rqr9rHrilRa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyhealth.tokenizer import Tokenizer\n",
        "\n",
        "# мы используем список кодов ATC3 в качестве токена\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# инициализировать токенизатор\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "id": "ztx2KZEOlubu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de43868-98d5-4426-e18c-0b964bf68bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1d кодирование\n",
        "tokens = ['A03C', 'A03D', 'A03E', 'A03F', 'A04A', 'A05A', 'A05B', 'B035', 'C129']\n",
        "indices = tokenizer.convert_tokens_to_indices(tokens)\n",
        "print (indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puj86idKnTJK",
        "outputId": "f1936986-eff6-4199-b7b7-5c0631131729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 9, 10, 11, 12, 13, 14, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1d декодирование\n",
        "indices = [0, 1, 2, 3, 4, 5]\n",
        "tokens = tokenizer.convert_indices_to_tokens(indices)\n",
        "print (tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af3JLMCJnt6f",
        "outputId": "95e2e938-a919-4f5b-e362-cb0ad1e9942c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', 'A01A', 'A02A', 'A02B', 'A02X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Пример 2: 2D токенизатор**\n",
        "- Мы предоставляем примеры 2d-преобразования между токенами и индексами.\n",
        "- Мы используем `[\"<pad>\", \"<unk>\"]` как два специальных токена, `<pad>`используется для заполнения в многомерном кодировании, и `<unk>` используется для неизвестных токенов."
      ],
      "metadata": {
        "id": "LB-ASpH1oSKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# из pyhealth.tokenizer импортировать токенизатор\n",
        "\n",
        "# мы используем список кодов ATC3 в качестве токена\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# инициализировать токенизатор\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E-hbGxqoNPu",
        "outputId": "2d6fa834-b4a4-4a3e-a835-e2023b6d4234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of tokens to convert to indices.\n",
        "padding (default: True): whether to pad the tokens to the max number of tokens in the batch (smart padding).\n",
        "truncation (default: True): whether to truncate the tokens to max_length.\n",
        "max_length (default: 512): maximum length of the tokens. This argument is ignored if truncation is False.\n",
        "\"\"\"\n",
        "\n",
        "# 2d кодирование\n",
        "tokens = [\n",
        "    ['A03C', 'A03D', 'A03E', 'A03F'],\n",
        "    ['A04A', 'B035', 'C129']\n",
        "]\n",
        "\n",
        "# случай 1: по умолчанию с использованием заполнения, усечения и максимальной длины — 512\n",
        "indices = tokenizer.batch_encode_2d(tokens)\n",
        "print ('case 1:', indices)\n",
        "\n",
        "# случай 2: без заполнения\n",
        "indices = tokenizer.batch_encode_2d(tokens, padding=False)\n",
        "print ('case 2:', indices)\n",
        "\n",
        "# случай 3: усечение с max_length равно 3\n",
        "indices = tokenizer.batch_encode_2d(tokens, max_length=3)\n",
        "print ('case 3:', indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eYn_Fyof1m",
        "outputId": "3537d38a-3dd2-49e3-d365-207b8cbdafe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[8, 9, 10, 11], [12, 1, 1, 0]]\n",
            "case 2: [[8, 9, 10, 11], [12, 1, 1]]\n",
            "case 3: [[9, 10, 11], [12, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of indices to convert to tokens.\n",
        "padding (default: False): whether to keep the padding tokens from the tokens.\n",
        "\"\"\"\n",
        "\n",
        "# 2d декодирование\n",
        "indices = [\n",
        "    [8, 9, 10, 11],\n",
        "    [12, 1, 1, 0]\n",
        "]\n",
        "\n",
        "# случай 1: по умолчанию без заполнения\n",
        "tokens = tokenizer.batch_decode_2d(indices)\n",
        "print ('case 1:', tokens)\n",
        "\n",
        "# случай 2: использовать отступы\n",
        "tokens = tokenizer.batch_decode_2d(indices, padding=True)\n",
        "print ('case 2:', tokens)"
      ],
      "metadata": {
        "id": "gJBTSTsNougH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bd8104-3298-415e-9582-a73f46f8fb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>']]\n",
            "case 2: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>', '<pad>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **пример 3: 3D токенизатор**\n",
        "- Мы предоставляем примеры 3d-преобразования между токенами и индексами.\n",
        "- Мы используем `[\"<pad>\", \"<unk>\"]` как два специальных токена, `<pad>` используется для заполнения при многомерном кодировании и декодировании, и `<unk>` используется для неизвестных токенов."
      ],
      "metadata": {
        "id": "3JKwOFz_qa-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# из pyhealth.tokenizer импортировать токенизатор\n",
        "\n",
        "# мы используем список кодов ATC3 в качестве токена\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# инициализировать токенизатор\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "id": "HeSOeMX5qc5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56e8f7d-1178-4964-d7dd-dfa3651b7149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of lists of tokens to convert to indices.\n",
        "padding (default: (True, True)): a tuple of two booleans indicating whether to pad the tokens to the max number of tokens\n",
        "    and visits (smart padding).\n",
        "truncation (default: (True, True)): a tuple of two booleans indicating whether to truncate the tokens to the corresponding\n",
        "    element in max_length\n",
        "max_length (default: (10, 512)): a tuple of two integers indicating the maximum length of the tokens along the first and\n",
        "    second dimension. This argument is ignored if truncation is False.\n",
        "\"\"\"\n",
        "\n",
        "# 3d кодирование\n",
        "tokens = [\n",
        "    [\n",
        "        ['A03C', 'A03D', 'A03E', 'A03F'],\n",
        "        ['A08A', 'A09A'],\n",
        "    ],\n",
        "    [\n",
        "        ['A04A', 'B035', 'C129'],\n",
        "    ]\n",
        "]\n",
        "\n",
        "# случай 1: по умолчанию с использованием заполнения, усечения и максимальной длины — 512\n",
        "indices = tokenizer.batch_encode_3d(tokens)\n",
        "print ('case 1:', indices)\n",
        "\n",
        "# случай 2: отсутствие заполнения в первом измерении\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(False, True))\n",
        "print ('case 2:', indices)\n",
        "\n",
        "# случай 3: отсутствие заполнения во втором измерении\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(True, False))\n",
        "print ('case 3:', indices)\n",
        "\n",
        "# случай 4: без заполнения в обоих измерениях\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(False, False))\n",
        "print ('case 4:', indices)\n",
        "\n",
        "# случай 5: усечение с max_length равно (2,2) в обоих измерениях\n",
        "indices = tokenizer.batch_encode_3d(tokens, max_length=(2,2))\n",
        "print ('case 5:', indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2z8td_pqel2",
        "outputId": "b8c9a9f7-7fed-46d0-cf8f-880eac7b1429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0], [0, 0, 0, 0]]]\n",
            "case 2: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0]]]\n",
            "case 3: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1], [0]]]\n",
            "case 4: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1]]]\n",
            "case 5: [[[10, 11], [24, 25]], [[1, 1], [0, 0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of indices to convert to tokens.\n",
        "padding (default: False): whether to keep the padding tokens from the tokens.\n",
        "\"\"\"\n",
        "\n",
        "# 3d декодирование\n",
        "indices = [\n",
        "    [\n",
        "        [8, 9, 10, 11],\n",
        "        [24, 25, 0, 0]\n",
        "    ],\n",
        "    [\n",
        "        [12, 1, 1, 0],\n",
        "        [0, 0, 0, 0]\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n",
        "# случай 1: по умолчанию без заполнения\n",
        "tokens = tokenizer.batch_decode_3d(indices)\n",
        "print ('case 1:', tokens)\n",
        "\n",
        "# случай 2: использовать отступы\n",
        "tokens = tokenizer.batch_decode_3d(indices, padding=True)\n",
        "print ('case 2:', tokens)"
      ],
      "metadata": {
        "id": "hF2JRaqNrELU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bb0191-b2b2-4d74-90dd-63005794d13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A']], [['A04A', '<unk>', '<unk>']]]\n",
            "case 2: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A', '<pad>', '<pad>']], [['A04A', '<unk>', '<unk>', '<pad>'], ['<pad>', '<pad>', '<pad>', '<pad>']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNoA4XRyr3Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.\n",
        "\n",
        "Thanks very much for your support!"
      ],
      "metadata": {
        "id": "D00JD2HpbhL3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}