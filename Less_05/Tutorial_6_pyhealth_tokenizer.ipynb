{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximTislenko/DS_in_MED/blob/main/Less_05/Tutorial_6_pyhealth_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparation**\n",
        "- install pyhealth alpha version"
      ],
      "metadata": {
        "id": "PHPVEkPFKYkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbz1YJ2nqOSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143bda64-b39a-42a7-ec0a-01ebabfe9f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhealth\n",
            "  Downloading pyhealth-1.1.3-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pyhealth) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from pyhealth) (1.13.1+cu116)\n",
            "Collecting rdkit>=2022.03.4\n",
            "  Downloading rdkit-2022.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.3/29.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.2->pyhealth) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit>=2022.03.4->pyhealth) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->pyhealth) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.2->pyhealth) (1.15.0)\n",
            "Installing collected packages: rdkit, pyhealth\n",
            "Successfully installed pyhealth-1.1.3 rdkit-2022.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pyhealth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instruction on [pyhealth.tokenizer](https://pyhealth.readthedocs.io/en/latest/api/tokenizer.html)**\n",
        "- **[README]**: tokenizer is used for transformations between string-based tokens and integer-based indices, based on the overall token space. We provide flexible functions to tokenize 1D, 2D and 3D lists. **This module can be used in many other scenarios.**\n",
        "\n",
        "- **[Arguments]**:\n",
        "  - `tokens`: List of tokens in the vocabulary.\n",
        "  - `special_tokens`: List of special tokens to add to the vocabulary. (e.g., `<pad>`, `<unk>`). If not provided, no special tokens are added.\n",
        "\n",
        "- **[Functionality]**:\n",
        "  - `get_vocabulary_size`: Return the size of the vocabulary\n",
        "  - `convert_tokens_to_indices`: 1d conversion from tokens to indices\n",
        "  - `convert_indices_to_tokens`: 1d conversion from indices to tokens\n",
        "  - `batch_encode_2d`: 2d conversion from tokens to indices\n",
        "  - `batch_decode_2d`: 2d conversion from indices to tokens\n",
        "  - `batch_encode_3d`: 3d conversion from tokens to indices\n",
        "  - `batch_decode_3d`: 3d conversion from indices to tokens"
      ],
      "metadata": {
        "id": "_1S5rqae7FhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 1: 1D tokenization**\n",
        "- We provide examples for 1d transformation between tokens and indices\n",
        "- We use `[\"<pad>\", \"<unk>\"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens. In 1d tokenization, the `<pad>` token is not useful."
      ],
      "metadata": {
        "id": "rqr9rHrilRa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyhealth.tokenizer import Tokenizer\n",
        "\n",
        "# we use a list of ATC3 code as the token\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# initialize the tokenizer\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "id": "ztx2KZEOlubu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de43868-98d5-4426-e18c-0b964bf68bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1d encode\n",
        "tokens = ['A03C', 'A03D', 'A03E', 'A03F', 'A04A', 'A05A', 'A05B', 'B035', 'C129']\n",
        "indices = tokenizer.convert_tokens_to_indices(tokens)\n",
        "print (indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puj86idKnTJK",
        "outputId": "f1936986-eff6-4199-b7b7-5c0631131729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 9, 10, 11, 12, 13, 14, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1d decode\n",
        "indices = [0, 1, 2, 3, 4, 5]\n",
        "tokens = tokenizer.convert_indices_to_tokens(indices)\n",
        "print (tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af3JLMCJnt6f",
        "outputId": "95e2e938-a919-4f5b-e362-cb0ad1e9942c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', 'A01A', 'A02A', 'A02B', 'A02X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 2: 2D tokenization**\n",
        "- We provide examples for 2d transformation between tokens and indices\n",
        "- We use `[\"<pad>\", \"<unk>\"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens."
      ],
      "metadata": {
        "id": "LB-ASpH1oSKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyhealth.tokenizer import Tokenizer\n",
        "\n",
        "# we use a list of ATC3 code as the token\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# initialize the tokenizer\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E-hbGxqoNPu",
        "outputId": "2d6fa834-b4a4-4a3e-a835-e2023b6d4234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of tokens to convert to indices.\n",
        "padding (default: True): whether to pad the tokens to the max number of tokens in the batch (smart padding).\n",
        "truncation (default: True): whether to truncate the tokens to max_length.\n",
        "max_length (default: 512): maximum length of the tokens. This argument is ignored if truncation is False.\n",
        "\"\"\"\n",
        "\n",
        "# 2d encode\n",
        "tokens = [\n",
        "    ['A03C', 'A03D', 'A03E', 'A03F'],\n",
        "    ['A04A', 'B035', 'C129']\n",
        "]\n",
        "\n",
        "# case 1: default using padding, truncation and max_length is 512\n",
        "indices = tokenizer.batch_encode_2d(tokens)\n",
        "print ('case 1:', indices)\n",
        "\n",
        "# case 2: no padding\n",
        "indices = tokenizer.batch_encode_2d(tokens, padding=False)\n",
        "print ('case 2:', indices)\n",
        "\n",
        "# case 3: truncation with max_length is 3\n",
        "indices = tokenizer.batch_encode_2d(tokens, max_length=3)\n",
        "print ('case 3:', indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eYn_Fyof1m",
        "outputId": "3537d38a-3dd2-49e3-d365-207b8cbdafe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[8, 9, 10, 11], [12, 1, 1, 0]]\n",
            "case 2: [[8, 9, 10, 11], [12, 1, 1]]\n",
            "case 3: [[9, 10, 11], [12, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of indices to convert to tokens.\n",
        "padding (default: False): whether to keep the padding tokens from the tokens.\n",
        "\"\"\"\n",
        "\n",
        "# 2d decode\n",
        "indices = [\n",
        "    [8, 9, 10, 11],\n",
        "    [12, 1, 1, 0]\n",
        "]\n",
        "\n",
        "# case 1: default no padding\n",
        "tokens = tokenizer.batch_decode_2d(indices)\n",
        "print ('case 1:', tokens)\n",
        "\n",
        "# case 2: use padding\n",
        "tokens = tokenizer.batch_decode_2d(indices, padding=True)\n",
        "print ('case 2:', tokens)"
      ],
      "metadata": {
        "id": "gJBTSTsNougH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bd8104-3298-415e-9582-a73f46f8fb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>']]\n",
            "case 2: [['A03C', 'A03D', 'A03E', 'A03F'], ['A04A', '<unk>', '<unk>', '<pad>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 3: 3D tokenization**\n",
        "- We provide examples for 3d transformation between tokens and indices\n",
        "- We use `[\"<pad>\", \"<unk>\"]` as two special tokens, `<pad>` is used for padding in higher dimensional encoding and decoding, and `<unk>` is used for unknown tokens."
      ],
      "metadata": {
        "id": "3JKwOFz_qa-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyhealth.tokenizer import Tokenizer\n",
        "\n",
        "# we use a list of ATC3 code as the token\n",
        "token_space = ['A01A', 'A02A', 'A02B', 'A02X', 'A03A', 'A03B', 'A03C', 'A03D', 'A03E', \\\n",
        "          'A03F', 'A04A', 'A05A', 'A05B', 'A05C', 'A06A', 'A07A', 'A07B', 'A07C', \\\n",
        "          'A07D', 'A07E', 'A07F', 'A07X', 'A08A', 'A09A', 'A10A', 'A10B', 'A10X', \\\n",
        "          'A11A', 'A11B', 'A11C', 'A11D', 'A11E', 'A11G', 'A11H', 'A11J', 'A12A', \\\n",
        "          'A12B', 'A12C', 'A13A', 'A14A', 'A14B', 'A16A']\n",
        "\n",
        "# initialize the tokenizer\n",
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "print(tokenizer.get_vocabulary_size())"
      ],
      "metadata": {
        "id": "HeSOeMX5qc5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56e8f7d-1178-4964-d7dd-dfa3651b7149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of lists of tokens to convert to indices.\n",
        "padding (default: (True, True)): a tuple of two booleans indicating whether to pad the tokens to the max number of tokens\n",
        "    and visits (smart padding).\n",
        "truncation (default: (True, True)): a tuple of two booleans indicating whether to truncate the tokens to the corresponding\n",
        "    element in max_length\n",
        "max_length (default: (10, 512)): a tuple of two integers indicating the maximum length of the tokens along the first and\n",
        "    second dimension. This argument is ignored if truncation is False.\n",
        "\"\"\"\n",
        "\n",
        "# 3d encode\n",
        "tokens = [\n",
        "    [\n",
        "        ['A03C', 'A03D', 'A03E', 'A03F'],\n",
        "        ['A08A', 'A09A'],\n",
        "    ],\n",
        "    [\n",
        "        ['A04A', 'B035', 'C129'],\n",
        "    ]\n",
        "]\n",
        "\n",
        "# case 1: default using padding, truncation and max_length is 512\n",
        "indices = tokenizer.batch_encode_3d(tokens)\n",
        "print ('case 1:', indices)\n",
        "\n",
        "# case 2: no padding on the first dimension\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(False, True))\n",
        "print ('case 2:', indices)\n",
        "\n",
        "# case 3: no padding on the second dimension\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(True, False))\n",
        "print ('case 3:', indices)\n",
        "\n",
        "# case 4: no padding on both dimensions\n",
        "indices = tokenizer.batch_encode_3d(tokens, padding=(False, False))\n",
        "print ('case 4:', indices)\n",
        "\n",
        "# case 5: truncation with max_length is (2,2) on both dimension\n",
        "indices = tokenizer.batch_encode_3d(tokens, max_length=(2,2))\n",
        "print ('case 5:', indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2z8td_pqel2",
        "outputId": "b8c9a9f7-7fed-46d0-cf8f-880eac7b1429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0], [0, 0, 0, 0]]]\n",
            "case 2: [[[8, 9, 10, 11], [24, 25, 0, 0]], [[12, 1, 1, 0]]]\n",
            "case 3: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1], [0]]]\n",
            "case 4: [[[8, 9, 10, 11], [24, 25]], [[12, 1, 1]]]\n",
            "case 5: [[[10, 11], [24, 25]], [[1, 1], [0, 0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch: List of lists of indices to convert to tokens.\n",
        "padding (default: False): whether to keep the padding tokens from the tokens.\n",
        "\"\"\"\n",
        "\n",
        "# 3d decode\n",
        "indices = [\n",
        "    [\n",
        "        [8, 9, 10, 11],\n",
        "        [24, 25, 0, 0]\n",
        "    ],\n",
        "    [\n",
        "        [12, 1, 1, 0],\n",
        "        [0, 0, 0, 0]\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n",
        "# case 1: default no padding\n",
        "tokens = tokenizer.batch_decode_3d(indices)\n",
        "print ('case 1:', tokens)\n",
        "\n",
        "# case 2: use padding\n",
        "tokens = tokenizer.batch_decode_3d(indices, padding=True)\n",
        "print ('case 2:', tokens)"
      ],
      "metadata": {
        "id": "hF2JRaqNrELU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bb0191-b2b2-4d74-90dd-63005794d13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A']], [['A04A', '<unk>', '<unk>']]]\n",
            "case 2: [[['A03C', 'A03D', 'A03E', 'A03F'], ['A08A', 'A09A', '<pad>', '<pad>']], [['A04A', '<unk>', '<unk>', '<pad>'], ['<pad>', '<pad>', '<pad>', '<pad>']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNoA4XRyr3Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you find it useful, please give us a star ⭐ (fork, and watch) at https://github.com/sunlabuiuc/PyHealth.\n",
        "\n",
        "Thanks very much for your support!"
      ],
      "metadata": {
        "id": "D00JD2HpbhL3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}